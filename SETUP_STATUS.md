# üöÄ Local AI Beast - Setup Status

## ‚úÖ Completed Steps

1. **‚úÖ Ollama Models Installed** (6 models, ~22.5GB)
   - `qwen3:4b` - Fast chat (2.5GB)
   - `qwen3:8b` - Balanced (5.2GB)
   - `deepseek-r1:8b` - Reasoning (5.2GB)
   - `qwen2.5-coder:7b` - Coding (4.7GB)
   - `llava:7b` - Vision (4.7GB)
   - `nomic-embed-text` - Embeddings (274MB)

2. **‚úÖ Open WebUI Downloaded**
   - Location: `C:\Users\jcgus\Documents\beastAI\open-webui\`

3. **‚úÖ Python 3.12 Installed**
   - Version: Python 3.12.10

4. **‚úÖ Backend Dependencies Installed**
   - Virtual environment: `open-webui\backend\venv\`
   - All Python packages installed successfully

---

## ‚è≥ Pending Steps

### 1. Install Node.js (Required for Frontend)

**Option A: Via winget (needs admin approval)**
```powershell
# Run PowerShell as Administrator, then:
winget install --id OpenJS.NodeJS.LTS -e --source winget
```

**Option B: Manual Download**
1. Visit: https://nodejs.org/
2. Download Node.js LTS (v20.x or v22.x)
3. Run installer
4. Restart terminal after installation

**Verify Installation:**
```powershell
node --version  # Should show v20.x.x or v22.x.x
npm --version   # Should show 10.x.x or 11.x.x
```

### 2. Install Frontend Dependencies

Once Node.js is installed:
```powershell
cd C:\Users\jcgus\Documents\beastAI\open-webui
npm install
```

### 3. Configure Open WebUI

Create `.env` file in `open-webui\backend\`:
```env
OLLAMA_BASE_URL=http://localhost:11434
WEBUI_AUTH=false
ENABLE_RAG_WEB_SEARCH=true
RAG_EMBEDDING_MODEL=nomic-embed-text
```

### 4. Start the Services

**Terminal 1 - Backend:**
```powershell
cd C:\Users\jcgus\Documents\beastAI\open-webui\backend
.\venv\Scripts\Activate.ps1
python -m open_webui.main
```

**Terminal 2 - Frontend:**
```powershell
cd C:\Users\jcgus\Documents\beastAI\open-webui
npm run dev
```

**Access:** http://localhost:3000

---

## üìã Quick Start Scripts

I'll create startup scripts once Node.js is installed.

---

## üîç Troubleshooting

### Ollama Not Running
```powershell
ollama serve
```

### Port Already in Use
- Backend default: `8080`
- Frontend default: `3000`
- Change in `.env` or `vite.config.ts`

### Models Not Showing
- Ensure Ollama is running: `ollama list`
- Check `OLLAMA_BASE_URL` in `.env`

---

**Last Updated:** December 5, 2025

